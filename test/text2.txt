
登录 | 注册
笑笑的程序人生
不以物喜，不以己悲
目录视图摘要视图订阅
 一种没有语料字典的分词方法
标签： 算法python文字处理大数据
2013-05-14 16:38 2712人阅读 评论(1) 收藏 举报
 分类： 技术（21）   算法（14）  
版权声明：本文为博主原创文章，未经博主允许不得转载。
前几天在网上闲逛，看到一篇美文，说的是怎么在没有语料库的情况下从文本中提取中文词汇，理论部分讲得比较多，但都还是很浅显易懂的，其中涉及一部分信息论的理论，其实只要大学开过信息论这门课的话，看起来还是挺简单的。

信息论我忘得差不多了，但是其中主要的内容还记得，信息论最主要的就是信息其实是可以度量的，一个事件包含的信息和它发生的概率成反比，简单的说，同样一个事件，产生A结果的概率为Pa，产生B结果的概率为Pb，如果Pa大于Pb，那A所包含的信息量就比B要大。

打个简单的比喻，比如中国队和西班牙队踢足球比赛，大家都知道，西班牙队赢的概率大概是99.9999%，中国队赢的概率大概是0.0001%，假如最后的结果是中国队赢了（靠实力）的话，那这个事件（中国队赢了西班牙）就是个信息量非常巨大的事件，我相信各大报纸的头条都会报道，反而如果西班牙赢了，估计没有报纸会报道这个消息，这就是信息论的核心，也就是信息熵。

扯远了，上面只是说说我对信息论的理解，在分词技术上，目前的分词技术基本上都是基于字典的，就是看文章中有没有字典含有的词语，如果有，就把这个当成一个词来分，这样也衍生了很多分词技术，大家有兴趣可以自己去查一查。

如果我们没有词典，需要分词就比较麻烦了，假设我们最大的词长度设定为5，能想到的办法就是，从第一个字开始，把文本中所有连在一起的两个字，三个字，四个字，五个字的片段找出来，然后看这些片段在文本中出现的频率，频率高的就当成一个词汇，这样确实能分出词来，但是这样同样也分出了一些不是词的词汇，比如上面的文章中，“的话”这个词就出现的相对比较多，显然，这不是一个词。

在这种基础上怎么把类似“的话”这样的词去掉就是我们要做的工作了。这涉及到两个重要的部分，也是那篇美文提到的两个部分。

第一，怎么去掉类似于“的中国”和“中国队”这样的差异，这三个词可能出现的频率都差不多，甚至“的中国”出现的频率更高，假设对于一个长文本（假设为10000，并且是描述足球比赛的），
“的中国”出现了100次，概率是1%
“中国队”出现了60次，概率是0.6%
“中国”   出现了200次，概率为2%
“的”       出现了500次，概率为5%
“队”       出现了70次，概率为0.7%
这样的数据情况下，“的”字和“中国”随机组合在一起的概率为5%*2%=1%，而“队”和“中国”随机组合在一起的概率为0.7%*2%=0.14%，显然“的中国”的出现概率和他们组合在一起的概率差不多，所以我们认为“的中国”更像是随机组合在一起的词而不是一个固定的词汇，但“中国队”出现的概率比他们组合在一起的概率高了4.28倍，所以我们认为“中国队”更像一个词汇。
通过上面的计算，我们就可以把“的中国”这样的词丢掉了，就算他出现了很多次，但是我们一样不认为它是一个词，这就是第一部分，我们把它叫词语的凝聚程度，“的中国”显然凝聚力不够。


第二，像类似“了一”这样的词在文章中肯定也出现得很多，因为是足球比赛文章，所以经常出现摔了一跤，进了一球等等，显然，“了一”也不是一个词汇，但是单独看“了”和“一”，“了一”的概率可能比他们单独组合的概率高不少，像这样的词怎么弄呢，这就要用到开头我们讲的信息论中的信息熵了。

“信息熵”能够反映知道一个事件的结果后平均会给你带来多大的信息量。如果某个结果的发生概率为 p ，当你知道它确实发生了，你得到的信息量就被定义为 - log(p) 。 p 越小，你得到的信息量就越大。在“了一”这个词中，他的前缀为“摔”，“进”，概率分别是p1,p2那他的前缀信息熵就是- log(p1)*p1-log(p2)*p2，同样后缀的信息熵也可以算出来，算出来以后我们取其中较小的那个作为这个词的前后缀信息熵，结果算出来是一个非常小的数，比如0.2，比普通真实的词汇，比如“摔了一跤”这样的词的信息熵都要小，所以显然可能“了一”就不是一个词汇了，反而“摔了一跤”这样的词更像一个词汇。

有了这两个理论，那我们找出一个词，然后按这两个条件给出一个阈值，就能抽取出一批我们认为是词汇的词语了，不需要字典，而且从抽取出来的词根据词频排列的话，再把一般性的词如“应该”，“如果”，“似乎”这样的介词去掉的话，一般是这个文本的关键字集合了。

嗯，理论部分大概就是这样了，具体的大家要是有兴趣，可以看看这篇blog，我都是看到这个的，顺便推荐一个这个作者，他的文章基本上都是原创，而且对数学理解得很到位，很适合各种程序猿们，呵呵。
http://www.matrix67.com/blog/archives/5044

按照这个东西，我自己写了个python脚本，写得很着急，而且不知道理论理解得对不对，后来我对《明朝那些事》抽了一把，出现了这些：
危险 优秀沉默李景隆影响
杨继盛 痛苦  脑袋 徐有贞厉害支持蓝玉杨廷和瓦剌申时行
判断 记载  孙承宗坚持愤怒奏疏
锦衣卫 严世蕃
祁钰 倭寇  东林党麻烦朋友
努尔哈赤  选择喜欢容易
首辅 御史李如松父亲戚继光投降
陈友谅 厚照  希望 蒙古简单估计消息彻底
胡宗宪 祁镇毕竟魏忠贤告诉袁崇焕嘉靖似乎准备。。。。。。。。

而对于《时间简史》，出现了这些：
空间时间 吸引增加基本尺度
效应 东西描述轨道
位置 边界产生解释夸克
广义相对论坍缩
辐射 部分
知道 状态区域距离爆炸
问题 奇点模型太阳
事件 科学预言运动膨胀
任何 必须恒星黑洞.......

你修改约束条件，就会出现不同的词语，怎么平衡这个约束条件也是个问题。

另外，如果你有兴趣，可以分析分析一些人的blog和微博之类的，再和你自己的对比一下，说不定能找到很多关键词一样哦，还有，要是你有海量的数据，也可以做一些非常有意思的事情哦，哈哈。


最后，再次推荐一下matirx67的blog:  http://www.matrix67.com/     


程序做得太快了，不好，还是把关键函数贴出来吧，就是暴力的编码，用了python自带的方便的数据结构来快速开发。所以速度上比较慢，而且内存上。。呵呵。。说不定会爆哦。。。。完整程序：
https://github.com/wyh267/ChineseWordSegmentation

[python] view plain copy print?
# -*- coding=utf-8 -*-  
''''' 
Created on 2013-5-9 
 
@author: Wu YingHao 
 
@email:  wyh817@gmail.com 
 
该程序可以在没有语料库的情况下从文本中抽取出中文词汇 
理论支持： 
http://www.matrix67.com/blog/archives/5044 
 
'''  
  
  
import math  
  
  
""" 
    计算每个字和词的出现频率 
    输入： words 字符串内容 
      num 需要截取的最长字串 
    输出： split_words 分割好的所有子串的数据，以字典形式返回 
      split_words 说明 
      {"字串" : [ 出现次数,出现概率,凝固程度,凝固程度*出现次数,自由程度,前缀集合,后缀集合] .....} 
"""  
def find_words(words,num=6):  
    split_words={}  
    lens=len(words)  
    for i in range(0,lens):  
        for j in range(1,num+1):  
            if i+j < lens -num-2:  
                if words[i:i+j] in split_words:  
                    split_words[words[i:i+j]][0]+=1  
                    split_words[words[i:i+j]][1]=float(split_words[words[i:i+j]][0])/float(lens)  
                    split_words[words[i:i+j]][6].append(words[i-1])  
                    split_words[words[i:i+j]][7].append(words[i+j])  
                else:  
                    split_words[words[i:i+j]]=[1,  
                                               1/float(lens),  
                                               words[i:i+j],  
                                               1,  
                                               1,  
                                               0,  
                                               [words[i-1]],  
                                               [words[i+j]]  
                                               ]  
                      
                                                 
        if(i%10000==0):  
            print "完成 :" + str(float(i)/float(len(words))*100) + " %"  
                  
    return split_words  
                  
  
  
""" 
    计算凝聚程度 
    输入：words_dic 已经拆分好的字符串字典 
    输出：填充好凝聚程度的字典 
"""  
def find_nh(words_dic):  
    for key in words_dic.keys():  
        if(len(key)>1):  
            #左凝聚程度  
            left_p=words_dic[key][1]/(words_dic[key[:1]][1]*words_dic[key[1:]][1])  
            #右凝聚程度  
            right_p=words_dic[key][1]/(words_dic[key[:-1]][1]*words_dic[key[:-1]][1])  
                   
              
            if(left_p<right_p):  
                words_dic[key][3]=left_p  
            else:  
                words_dic[key][3]=right_p  
      
  
  
  
""" 
    计算自由程度 
    输入： word_dic 字典文件 
    返回：word_dic 添加自由程度以后的字典 
"""  
def calc_free(word_dic):  
    for key in word_dic.keys():  
        front_free=0  
        end_free=0  
        for front in word_dic[key][6]:  
            if front in word_dic:  
                front_free-=math.log(word_dic[front][1])*word_dic[front][1]  
          
        for end in word_dic[key][7]:  
            if end in word_dic:  
                end_free-=math.log(word_dic[end][1])*word_dic[end][1]  
              
        if(front_free < end_free):  
            word_dic[key][5]=front_free  
        else:  
            word_dic[key][5]=end_free  
  
    return word_dic  
<span style="font-family: Arial, Helvetica, sans-serif;">    </span>  


注：本文虽然不是转帖，但是内容参考了matrix67的博客，地址为  http://www.matrix67.com/blog/archives/5044  ，特此声明。


===========================================================================================
欢迎大家关注我的微信号：XJJ267    西加加语言
或者扫描下面的二维码也行哦。



顶
0
踩
0
 
 
上一篇Node.js初哥(一)
下一篇Zbar函数库示例代码
我的同类文章
技术（21）  算法（14）
•用Golang写一个搜索引擎（0x02）2016-04-12阅读300
•从零开始，写一个搜索引擎 （0x00）2016-04-09阅读321
•来聊聊STL标准库（一）---allocators2013-10-23阅读1792
•Go语言语法汇总2013-09-16阅读5330
•回文字符串2013-07-24阅读3673
•Haskell笔记 （五） 高阶函数2013-07-16阅读1445
•从零开始，写一个搜索引擎 （0x01）2016-04-12阅读397
•一个go语言实现的短链接服务2015-06-16阅读1130
•Go语言简单的TCP编程2013-09-17阅读11558
•不可表达的数 --- 梅森数 庞果题目2013-08-29阅读1150
•Haskell笔记 （六）自定义Types2013-07-16阅读921
更多文章
参考知识库
img
Python知识库
7729关注|811收录
img
算法与数据结构知识库
910关注|2080收录
img
微信开发知识库
6332关注|500收录
猜你在找
Python算法实战视频课程--二叉树Python算法实战视频课程--图Python算法实战视频课程--栈的应用Python算法实战视频课程--队列的应用用redis 搭建大数据 热门排行榜，用户推荐系统
ngram模型中文语料实验step by step1-分词与统计ubuntu 下没有pthread库以及报undefined reference to pthread_create的解决方法统计分词无字典分词学习7 模型方法统计分词无字典分词学习4候选片段的过滤方法互信息过滤统计分词无字典分词学习5候选片段的过滤方法边界稳定性
查看评论
1楼 大愚若智_ 2015-07-03 22:35发表 [回复]

方法很好
不过用一段短文跑一下你的代码到了这一步：正在计算自由程度.....
词语频率。。就停住了
您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
核心技术类目
全部主题 Hadoop AWS 移动游戏 Java Android iOS Swift 智能硬件 Docker OpenStack VPN Spark ERP IE10 Eclipse CRM JavaScript 数据库 Ubuntu NFC WAP jQuery BI HTML5 Spring Apache .NET API HTML SDK IIS Fedora XML LBS Unity Splashtop UML components Windows Mobile Rails QEMU KDE Cassandra CloudStack FTC coremail OPhone CouchBase 云计算 iOS6 Rackspace Web App SpringSide Maemo Compuware 大数据 aptech Perl Tornado Ruby Hibernate ThinkPHP HBase Pure Solr Angular Cloud Foundry Redis Scala Django Bootstrap
个人资料
 访问我的空间 
ygrx
 
访问：266063次
积分：3020
等级： 
排名：第8335名
原创：53篇转载：1篇译文：0篇评论：167条
欢迎关注我微信
欢迎大家关注我的微信号：XJJ267 【西加加语言】，或者扫描下面二维码哦
不是每天都推送，隔几天推一次，关注技术和生活

关于我
一个已经过了而立之年的码农。
成长在湘江之畔
求学于麓山脚下
工作在帝都北京
之前一直做嵌入式方向的开发，后来慢慢转向了管理，中间有很长时间没有写过代码了，但是自己还是非常热爱代码的工作，后来，终于下决心转行到互联网做一个码农。
一个机缘巧合的机会，在一家成立了很长时间的电商网站负责整个网站的搜索和排序系统，让我过了一个非常精彩的2014年，而现在，我又有新的征程了。
C/C++,go,Python,Obj-c是我的菜 hadoop和strome也还行偶尔也吃一些erlang,haskell。
算法也能聊聊，机器学习也能聊聊，搜索引擎也能聊聊，推荐系统也能聊聊，mips和arm也能聊聊，驱动程序也能聊聊，FPGA也能聊聊，呵呵，但都仅仅是聊聊啊。。。
平时就是看书，跑步，画画
GitHub:https://github.com/wyh267
欢迎交朋友: wyh817@gmail.com
文章分类
Direct3D学习笔记(4)
iphone开发(3)
linux(1)
杂谈(5)
c/c++(9)
技术(22)
Javascript(1)
算法(15)
云计算(4)
文章存档
2016年04月(3)
2015年06月(1)
2013年11月(2)
2013年10月(3)
2013年09月(8)
展开
阅读排行
[推荐算法]基于用户的协同过滤算法(34233)
如何在github上发起一个pull request(34200)
文本相似度计算-JaccardSimilarity和哈希签名函数(19546)
C++多线程框架（三）--------- 消息队列(12167)
Go语言简单的TCP编程(11558)
LWIP轻量级TCPIP协议栈的移植(11470)
Nginx一个IP配置多个主机(10795)
搭建自己的XenServer+CloudStack云平台，提供IaaS服务（一）环境搭建(10771)
搭建一个个人博客(8089)
C++多线程框架（一）--------- new一下就启动一个线程(6169)
评论排行
[推荐算法]基于用户的协同过滤算法(21)
字符串消除(21)
Direct3D学习笔记（三）画一个三角形出来吧(17)
Direct3D学习笔记（一）系统环境设置(15)
数组排序 --- 庞果(12)
Direct3D学习笔记（四） 制作一个真正的三维空间(8)
寻找直方图中面积最大的矩形 --- 庞果网(8)
关于技术(8)
C++多线程框架（三）--------- 消息队列(7)
搭建一个个人博客(7)
最新评论
LWIP轻量级TCPIP协议栈的移植
h244259402: 我专门登录了来点了个赞,好久没有看到过如此清晰的文章了
[推荐算法]基于用户的协同过滤算法
chengchengwoheni: 博主，求你把源码和数据集发我一份，邮箱1147841113@qq.com
C++多线程框架（三）--------- 消息队列
EarlyBird_Dumbass: 非常不错的分享。
LRU Cache的简单c++实现
fourier307: 简单易懂
如何在github上发起一个pull request
bluewudi: 第一次使用GitHub for windows为上面的社区做点贡献。没fork，直接从人家的repo...
[推荐算法]基于用户的协同过滤算法
Memorycollector: 博主可以跑了。请问哪里可以更改用户呢？这样每次出来结果都和你一样啊0.0
[推荐算法]基于用户的协同过滤算法
Memorycollector: -.-楼主我跑不了啊。下载了数据集。模块也装了，不知道楼主你用的那个版本的texttable呢？
你用过哪些操作系统？
ygrx: @mkxzy:这么看来，你和我年龄应该差不多拉，刚过了30没多久吧。。呵呵
[推荐算法]基于用户的协同过滤算法
hhf457764906: 楼主你写的这篇文章真的是既幽默又浅显易懂，我刚照着这篇文章的思路在项目里面加入了推荐算法，谢谢~
你用过哪些操作系统？
流浪的毛蟹: 楼主真幸福，有个好爸爸我记得第一次接触电脑是在初中，那时候学校刚配了个电脑室。记得负责教我们电脑知识...
公司简介|招贤纳士|广告服务|银行汇款帐号|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-600-2320|北京创新乐知信息技术有限公司 版权所有|江苏乐知网络技术有限公司 提供商务支持
京 ICP 证 09002463 号|Copyright © 1999-2016, CSDN.NET, All Rights Reserved GongshangLogo

